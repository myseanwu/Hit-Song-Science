{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType,StringType, DoubleType,NumericType,IntegerType\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import udf, array\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql.functions import lit\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all with only cols needed\n",
    "def union_df(df_1,df_2):\n",
    "    select_1 = df_1.select(\"created_at\", \"id\", \"truncated\", \"lang\",\n",
    "                       col(\"user.id\").alias(\"user_id\"), \"user.followers_count\", \"user.friends_count\", \"user.listed_count\",\n",
    "                       \"text\", col(\"entities.hashtags.text\").alias('hashtags'), \n",
    "                       \"entities.urls.display_url\", \"entities.urls.expanded_url\", \"entities.urls.url\", \n",
    "                       col(\"entities.user_mentions.screen_name\").alias(\"mentions_screen_name\"),\n",
    "                       col(\"extended_tweet.full_text\").alias(\"et_full_text\"), \n",
    "                       col(\"extended_tweet.entities.hashtags.text\").alias(\"et_hashtags\"), \n",
    "                       col(\"extended_tweet.entities.urls.display_url\").alias(\"et_display_url\"), \n",
    "                       col(\"extended_tweet.entities.urls.expanded_url\").alias(\"et_expanded_url\"), \n",
    "                       col(\"extended_tweet.entities.urls.url\").alias(\"et_url\"), \n",
    "                       col(\"extended_tweet.entities.user_mentions.screen_name\").alias(\"et_mentions_screen_name\"),\n",
    "                       \"place.country\", \"place.country_code\", \"place.name\", \"place.place_type\",\n",
    "                       \"favorite_count\", \"reply_count\", \"retweet_count\", \"quote_count\",'place')\n",
    "    select_2 = df_2.select(\"created_at\", \"id\", \"truncated\", \"lang\",\n",
    "                       col(\"user.id\").alias(\"user_id\"), \"user.followers_count\", \"user.friends_count\", \"user.listed_count\",\n",
    "                       \"text\", col(\"entities.hashtags.text\").alias('hashtags'), \n",
    "                       \"entities.urls.display_url\", \"entities.urls.expanded_url\", \"entities.urls.url\", \n",
    "                       col(\"entities.user_mentions.screen_name\").alias(\"mentions_screen_name\"),\n",
    "                       col(\"extended_tweet.full_text\").alias(\"et_full_text\"), \n",
    "                       col(\"extended_tweet.entities.hashtags.text\").alias(\"et_hashtags\"), \n",
    "                       col(\"extended_tweet.entities.urls.display_url\").alias(\"et_display_url\"), \n",
    "                       col(\"extended_tweet.entities.urls.expanded_url\").alias(\"et_expanded_url\"), \n",
    "                       col(\"extended_tweet.entities.urls.url\").alias(\"et_url\"), \n",
    "                       col(\"extended_tweet.entities.user_mentions.screen_name\").alias(\"et_mentions_screen_name\"),\n",
    "                       \"place.country\", \"place.country_code\", \"place.name\", \"place.place_type\",\n",
    "                       \"favorite_count\", \"reply_count\", \"retweet_count\", \"quote_count\",'place')\n",
    "\n",
    "    result = select_1.union(select_2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter songs and artists\n",
    "\n",
    "song_name = {\n",
    "    'Lemonade','ROCKSTAR','WHATS POPPIN','Come & Go','POPSTAR','Smile','Hit Different','Savage Love',\n",
    "    'Wolves',\"my ex's best friend\",\"Go Crazy\",\"GREECE\",\"Roses\",\"Ice Cream\",\"Hawái\",\"you broke me first\",\n",
    "    \"Lithuania\",\"Be Like That\",\"Lucid Dreams\",\"Blastoff\",\"DOLLAZ ON MY HEAD\",\"Robbery\",\"All Girls Are The Same\",\n",
    "    \"Sunflower - Spider-Man: Into the Spider-Verse\",\"Sunflower\",\"Hate The Other Side\",\"We Paid\",\n",
    "    \"Lets Link\",\"death bed\",\"Electric Love\",\"Breaking Me\",\"Rain On Me\",\"Still Don't Know My Name\",\n",
    "    \"Girls in the Hood\",\"Someone You Loved\",\"GO\",\"Rags2Riches 2\",\"cardigan\",\"Flex\",\"Over Now\",\"UN DIA\",\n",
    "    \"goosebumps\",\"Sunday Best\",\"hot girl bummer\",\"Whiskey Glasses\",\"THE SCOTTS\",\"Savage Remix\",\"Dior\",\n",
    "    \"Mad at Disney\",\"ZTFO\",\"Prospect\",\"my future\",\"HIGHEST IN THE ROOM\",\"exile\",\"Bandit\",\"Del Mar\",\n",
    "    \"ROXANNE\",\"Falling\",\"Are You Bored Yet?\",\"the 1\",\"Stunnin'\",\"Ballin'\",\"Walk Em Down\",\"Deep Reverence\",\n",
    "    \"Money Trees\",\"Chicago Freestyle\",\"My Window\",\"Conversations\",\"OK Not To Be OK\",\"Righteous\",\n",
    "    \"SICKO MODE\",\"What You Know Bout Love\",\"Toosie Slide\",\"Head & Heart\",\"Put Your Records On\",\n",
    "    \"You Got It\",\"Jocelyn Flores\",\"Holy\",\"Diamonds\",\"The Box\",\"Daisy\",\"September\",\"If the World Was Ending\",\n",
    "    \"SLOW DANCING IN THE DARK\",\"FRANCHISE\",\"forget me too\",\"Ew\",\"Your Man\",\"Epidemic\",\"Gimme Love\",\n",
    "    \"bloody valentine\",\"Daylight\",\"MODUS\",\"drunk face\",\"Tick Tock\",\"Better\",\"Sanctuary\",\"Run\",\n",
    "    \"Pretty Boy\",\"Got It On Me\",\"Afterthought\",\"all I know\",\"Like You Do\",\"kiss kiss\",\"title track\",\n",
    "    \"NITROUS\",\"Runnin\",\"Mr. Right Now\",\"Glock In My Lap\",\"Wonder\",\"Rich Nigga Shit\",\"Many Men\",\n",
    "    \"Slidin\",\"Bet You Wanna\",\"Lovesick Girls\",\"Outta Time\",\"Brand New Draco\",\"My Dawg\",\"Snitches & Rats\",\n",
    "    \"No Opp Left Behind\",\"Steppin On Niggas\",\"RIP Luv\",\"Don't Stop\",\"Sofia\",\"Said N Done\",\"Intro\",\n",
    "    \"After Party\",\"Pretty Savage\",\"Heart Of Glass\",\"How You Like That\",\"Intentions\",\"Crazy Over You\",\n",
    "    \"Sleepy Hollow\",\"Sweater Weather\",\"Levitating\",\"E-GIRLS ARE RUINING MY LIFE!\",\"Baby, I'm Jealous\",\n",
    "    \"Lonely\",\"Blueberry Faygo\",\"Hate The Way\",\"Moonwalking in Calabasas\",\"Whole Lotta Choppas\",\n",
    "    \"Train Wreck\",\"positions\",\"Forever After All\",\"Tyler Herro\",\"Spicy\",\"Beautiful Crazy\",\n",
    "    \"Beer Never Broke My Heart\",\"Life Is Good\",\"Back to the Streets\",\"Daddy Issues\",\"Falling\",\n",
    "    \"Chasin' You\",\"Golden\",\"Monster Mash\",\"Excitement\",\"Ghostbusters\",\"Took Her To The O\",\"Hawái - Remix\",\n",
    "    \"ALWAYS DO\",\"Took Her To The O\",\"TRAGIC\",\"Young Wheezy\",\"All I Want for Christmas Is You\",\"The Code\",\n",
    "    \"What That Speed Bout!?\",\"WITHOUT YOU\",\"F*CK YOU, GOODBYE\",\"Whoopty\",\"Rockin' Around The Christmas Tree\",\n",
    "    \"Therefore I Am\",\"Drankin N Smokin\",\"HOLIDAY\",\"Stripes Like Burberry\",\"That's It\",\"Jingle Bell Rock\",\n",
    "    \"Real Baby Pluto\",\"Last Christmas\",\"Santa Tell Me\",\"It's the Most Wonderful Time of the Year\",\n",
    "    \"It's Beginning to Look a Lot like Christmas\",\"Marni On Me\",\"Million Dollar Play\",\"Sleeping On The Floor\",\n",
    "    \"Plastic\",\"Underneath the Tree\",\"lovely\",\"Monster\",\"Somebody's Problem\",\"Life Goes On\",\"Prisoner\",\n",
    "    \"Body\",\"Still Goin Down\",\"Circles\",\"Fly To My Room\",\"Cry Baby\",\"Circles\",\"Blue & Grey\",\"Telepathy\",\n",
    "    \"Let It Snow! Let It Snow! Let It Snow!\",\"Dis-ease\",\"Sleigh Ride\",\"Let It Snow! Let It Snow! Let It Snow!\",\n",
    "    \"Mistletoe\",\"Feliz Navidad\",\"Holly Jolly Christmas\",\"Stay\",\"Livin' The Dream\",\"Hello\",\n",
    "    \"A Holly Jolly Christmas - Single Version\",\"A Holly Jolly Christmas\",\"Movie\",\"Line Without a Hook\",\n",
    "    \"The Christmas Song\",\"LA NOCHE DE ANOCHE\",\"TE MUDASTE\",\"YO VISTO ASÍ\",\"HACIENDO QUE ME AMAS\",\n",
    "    \"Wonderful Christmastime [Edited Version] - Remastered 2011 / Edited Version\",\"Wonderful Christmastime\",\n",
    "    \"TE DESEO LO MEJOR\",\"BOOKER T\",\"Do They Know It's Christmas? - 1984 Version\",\"Do They Know It's Christmas?\",\n",
    "    \"MALDITA POBREZA\",\"HOY cobré\",\"EL MUNDO ES MÍO\",\"White Christmas\",\"Angels Like You\",\"LA DROGA\",\n",
    "    \"It's Beginning to Look a Lot Like Christmas\",\"Christmas\",\"SORRY PAPI\",\"Blue Christmas\",\n",
    "    \"ANTES QUE SE ACABE\",\"120\",\"Run Rudolph Run - Single Version\",\"Run Rudolph Run\",\n",
    "    \"Santa Claus Is Coming To Town\",\"TRELLAS\",\"Baby, It's Cold Outside\",\"Happy Xmas\"\n",
    "    }\n",
    "\n",
    "artists = { \"Internet Money\",\"DaBaby\",\"Jack Harlow\",\"Juice WRLD\",\"BTS\",\"DJ Khaled\",\"SZA\",\"Jawsh 685\",\"Big Sean\",\n",
    "           \"Machine Gun Kelly\",\"Chris Brown\",\"DJ Khaled\",\"SAINt JHN\",\"BLACKPINK\",\"Maluma\",\"Tate McRae\",\n",
    "           \"Big Sean\",\"Kane Brown\",\"Internet Money\",\"Gunna\",\"Post Malone\",\"Lil Baby\",\"WhoHeem\",\"Powfu\",\n",
    "           \"BØRNS\",\"Topic\",\"Lady Gaga\",\"Labrinth\",\"Megan Thee Stallion\",\"The Kid LAROI\",\"Rod Wave\",\n",
    "           \"Taylor Swift\",\"Polo G\",\"Calvin Harris\",\"J Balvin\",\"Travis Scott\",\"Surfaces\",\"blackbear\",\n",
    "           \"Morgan Wallen\",\"THE SCOTTS\",\"Pop Smoke\",\"salem ilese\",\"iann dior\",\"Billie Eilish\",\"Ozuna\",\n",
    "           \"Arizona Zervas\",\"Trevor Daniel\",\"Wallows\",\"Curtis Waters\",\"Mustard\",\"NLE Choppa\",\"Kendrick Lamar\",\n",
    "           \"Drake\",\"YoungBoy Never Broke Again\",\"Marshmello\",\"Joel Corry\",\"Ritt Momney\",\"Vedo\",\n",
    "           \"XXXTENTACION\",\"Justin Bieber\",\"Sam Smith\",\"Roddy Ricch\",\"Ashnikko\",\"Joji\",\"Fleetwood Mac\",\n",
    "           \"ZAYN\",\"21 Savage\",\"Shawn Mendes\",\"Bryson Tiller\",\"Clairo\",\"Don Toliver\",\"Miley Cyrus\",\n",
    "           \"Trippie Redd\",\"The Neighbourhood\",\"Dua Lipa\",\"CORPSE\",\"Bebe Rexha\",\"Lil Mosey\",\"G-Eazy\",\n",
    "           \"DDG\",\"Sada Baby\",\"James Arthur\",\"Ariana Grande\",\"Luke Combs\",\"Jack Harlow\",\"Ty Dolla $ign\",\n",
    "           \"The Kid LAROI\",\"DaBaby\",\"Future\",\"Saweetie\",\"Harry Styles\",\"Bobby Pickett\",\n",
    "            \"Trippie Redd\",\"Ray Parker Jr.\",\"King Von\",\"The Kid LAROI\",\"NAV\",\"Mariah Carey\",\"CJ\",\n",
    "           \"Brenda Lee\",\"Billie Eilish\",\"Lil Nas X\",\"Bobby Helms\",\"Wham!\",\"Andy Williams\",\"Michael Buble\",\n",
    "           \"Kelly Clarkson\",\"Billie Eilish\",\"Dean Martin\",\"The Ronettes\",\"Frank Sinatra\",\"José Feliciano\",\n",
    "           \"Burl Ives\",\"Ricky Montgomery\",\"Nat King Cole\",\"Paul McCartney\",\"Bing Crosby\",\"Perry Como\",\n",
    "           \"Darlene Love\",\"Elvis Presley\",\"Chuck Berry\",\"The Jackson 5\",\"Brett Eldredge\",\"John Lennon\"\n",
    "}\n",
    "\n",
    "def find_songs(line):\n",
    "    for name in song_name:\n",
    "        if name.lower() in line.lower() and ('song' in line.lower() or 'music' in line.lower()):\n",
    "            return name\n",
    "\n",
    "def find_artist(line):\n",
    "    for artist in artists:\n",
    "        if artist.lower() in line.lower():\n",
    "            return artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter song related\n",
    "# # song\n",
    "# song_process = udf(find_songs, StringType())\n",
    "# DF = DF.withColumn('song_name',song_process('text'))\n",
    "\n",
    "# ## artist\n",
    "# artist_process = udf(find_artist, StringType())\n",
    "# DF = DF.withColumn('artist_name',artist_process('text'))\n",
    "\n",
    "# ## SQL\n",
    "# DF.createOrReplaceTempView(\"df\")\n",
    "# result = spark.sql(\"SELECT * FROM df WHERE ( song_name IS NOT NULL) OR (artist_name IS NOT NULL)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## all language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder = \"all_wk0\"\n",
    "# df0 = sqlContext.read.parquet(folder).select('created_at','id','lang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder = \"all_wk1\"\n",
    "# df1 = sqlContext.read.parquet(folder).select('created_at','id','lang')\n",
    "# folder = \"all_wk2\"\n",
    "# df2 = sqlContext.read.parquet(folder).select('created_at','id','lang')\n",
    "# folder = \"all_wk3\"\n",
    "# df3 = sqlContext.read.parquet(folder).select('created_at','id','lang')\n",
    "# folder = \"all_wk4\"\n",
    "# df4 = sqlContext.read.parquet(folder).select('created_at','id','lang')\n",
    "# folder = \"all_wk5\"\n",
    "# df5 = sqlContext.read.parquet(folder).select('created_at','id','lang')\n",
    "# folder = \"all_wk6\"\n",
    "# df6 = sqlContext.read.parquet(folder).select('created_at','id','lang')\n",
    "# folder = \"all_wk7\"\n",
    "# df7 = sqlContext.read.parquet(folder).select('created_at','id','lang')\n",
    "# folder = \"all_wk8\"\n",
    "# df8 = sqlContext.read.parquet(folder).select('created_at','id','lang')\n",
    "# folder = \"all_wk9\"\n",
    "# df9 = sqlContext.read.parquet(folder).select('created_at','id','lang')\n",
    "# folder = \"all_wk10\"\n",
    "# df10 = sqlContext.read.parquet(folder).select('created_at','id','lang')\n",
    "# folder = \"all_wk11\"\n",
    "# df11 = sqlContext.read.parquet(folder).select('created_at','id','lang')\n",
    "# folder = \"all_wk12\"\n",
    "# df12 = sqlContext.read.parquet(folder).select('created_at','id','lang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spotify, billboard in text?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read spotify files\n",
    "folder = 'all_spotify'\n",
    "DF = sqlContext.read.parquet(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding weeks for bins\n",
    "## \n",
    "from pyspark.sql.functions import udf, array\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType,StringType,NumericType,IntegerType\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "def find_month(date: str):   \n",
    "    month = date.split()[1]\n",
    "    if month == 'Sep':\n",
    "        m = 9\n",
    "    elif month == 'Oct':\n",
    "        m = 10\n",
    "    elif month == 'Nov':\n",
    "        m = 11\n",
    "    elif month == 'Dec':\n",
    "        m = 12\n",
    "    elif month == 'Aug':\n",
    "        m = 8\n",
    "    return m\n",
    "\n",
    "def find_day(date: str):\n",
    "    return int(date.split()[2])\n",
    "\n",
    "split_month = udf(find_month, IntegerType())\n",
    "split_day = udf(find_day, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = DF.withColumn('month',split_month('created_at'))\n",
    "D = D.withColumn('day',split_day('created_at'))\n",
    "D = D.withColumn('month_day', (D['month']*100 + D['day'])  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify\n",
    "# D.createOrReplaceTempView(\"df\")\n",
    "# result = spark.sql(\"SELECT month,day,month_day,COUNT(1) FROM df GROUP BY month,day,month_day HAVING month == 9  \")\n",
    "# result.show(31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tweets metioned only song_name vs only artist_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## only en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en = D.filter(D['lang']=='en')\n",
    "# df_en.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|weeks|count(1)|\n",
      "+-----+--------+\n",
      "|  0.0| 5718824|\n",
      "|  1.0| 5112478|\n",
      "|  2.0| 4612646|\n",
      "|  3.0| 4336484|\n",
      "|  4.0| 4742509|\n",
      "|  5.0| 5686053|\n",
      "|  6.0| 5512342|\n",
      "|  7.0| 4237039|\n",
      "|  8.0| 3962829|\n",
      "|  9.0| 4749645|\n",
      "| 10.0| 3988297|\n",
      "| 11.0| 4555603|\n",
      "| 12.0| 5584133|\n",
      "| 13.0|  568291|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# adding weeks\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "week_split = [0,904,911,918,925,1002,1009,1016,1023,1030,1106,1113,1120,1127,1203]\n",
    "buck = Bucketizer(inputCol = 'month_day'   , splits = week_split, outputCol='weeks')\n",
    "dfbins = buck.transform(D)\n",
    "# dfbins.select('created_at','weeks').show()\n",
    "# dfbins.show(3,truncate= True)\n",
    "\n",
    "dfbins.createOrReplaceTempView(\"df\")\n",
    "result = spark.sql(\"SELECT weeks,COUNT(1) FROM df GROUP BY weeks,lang HAVING lang='en' ORDER BY weeks \")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "week_split = [0,904,911,918,925,1002,1009,1016,1023,1030,1106,1113,1120,1127,1203]\n",
    "buck = Bucketizer(inputCol = 'month_day'   , splits = week_split, outputCol='weeks')\n",
    "\n",
    "df_en = D.filter(D['lang']=='en')\n",
    "dfbins_en = buck.transform(df_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfbins_en.columns\n",
    "ds = dfbins_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['created_at',\n",
       " 'id',\n",
       " 'truncated',\n",
       " 'lang',\n",
       " 'user_id',\n",
       " 'followers_count',\n",
       " 'friends_count',\n",
       " 'listed_count',\n",
       " 'text',\n",
       " 'hashtags',\n",
       " 'display_url',\n",
       " 'expanded_url',\n",
       " 'url',\n",
       " 'mentions_screen_name',\n",
       " 'et_full_text',\n",
       " 'et_hashtags',\n",
       " 'et_display_url',\n",
       " 'et_expanded_url',\n",
       " 'et_url',\n",
       " 'et_mentions_screen_name',\n",
       " 'country',\n",
       " 'country_code',\n",
       " 'name',\n",
       " 'place_type',\n",
       " 'favorite_count',\n",
       " 'reply_count',\n",
       " 'retweet_count',\n",
       " 'quote_count',\n",
       " 'place',\n",
       " 'song_name',\n",
       " 'artist_name',\n",
       " 'month',\n",
       " 'day',\n",
       " 'month_day',\n",
       " 'weeks']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfbins_en.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# avg length: \n",
    "> ### url, et_url, display_url, et_expanded_url, hashtags, et_hashtags\n",
    "> ### avg followers_count, friends_count, listed_count\n",
    "> ### trucated\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate emoji count \n",
    "import re\n",
    "from pyspark.sql.functions import udf, array\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType,StringType, ArrayType, FloatType\n",
    "\n",
    "def remove_stop(line):\n",
    "    #token = nltk.word_tokenize(line)\n",
    "    WORD_RE = re.compile(r\"[\\w']+\")\n",
    "    token = WORD_RE.findall(line)\n",
    "    tokens = [w.lower() for w in token if w.lower() not in STOPWORDS]\n",
    "    return tokens\n",
    "\n",
    "def find_emoji(line):\n",
    "    ## remove Korean chrachters i.e. 지, 민, 방, 탄\n",
    "    hangul = re.compile(u'[a-zA-Z0-9\\u3131-\\u3163\\uac00-\\ud7a3]+')  \n",
    "    line = re.sub(hangul, \"\", line) \n",
    "    \n",
    "    ## Japanese/hiragana\n",
    "    hiragana = re.compile(u'[\\u3040-\\u309Fー]+') # == u'[ぁ-んー]+'\n",
    "    line = re.sub(hiragana, \"\", line)\n",
    "    # Japanese/Katakana\n",
    "    Katakana = re.compile(u'[\\u30A0-\\u30FF]+') # == u'[ァ-ヾ]+'\n",
    "    line = re.sub(Katakana, \"\", line)\n",
    "    # Japanese/\n",
    "    Kanji = re.compile(u'[\\u4E00-\\u9FFF]+') # == u'[一-龠々]+'\n",
    "    line = re.sub(Kanji, \"\", line)\n",
    "    # find emoji\n",
    "    regex = re.compile(r'([\\u263a-\\U0001f645])')\n",
    "    token_list = regex.findall(line)\n",
    "    \n",
    "    return token_list\n",
    "\n",
    "# UDF\n",
    "emoji = udf(find_emoji, ArrayType(StringType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as _sum\n",
    "from pyspark.sql import functions as F\n",
    "# tokenize\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "def length(line):\n",
    "    try:\n",
    "        return len(line)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def find_truncated(line):\n",
    "    if line == True:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def find_http_in_tweets(text:str):\n",
    "    if 'http' in text:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "    \n",
    "# UDF \n",
    "find_counts = udf(length, IntegerType())\n",
    "find_truncated = udf(find_truncated, IntegerType())\n",
    "find_http = udf(find_http_in_tweets, IntegerType())\n",
    "# UDF counting words\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "countChars = udf(lambda text: len(text), IntegerType())\n",
    "\n",
    "\n",
    "# add col\n",
    "T = ds.select(['weeks','song_name','artist_name','url','et_url','display_url','et_expanded_url',\n",
    "               'hashtags','et_hashtags','followers_count','friends_count','listed_count',\n",
    "               'followers_count','truncated','text'])\n",
    "\n",
    "## Regextokenize by regex\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "regexTokenized = regexTokenizer.transform(T)\n",
    "T = regexTokenized.withColumn(\"tokens\", countTokens(col(\"words\"))\n",
    "                             ).withColumn(\"char_count\",countChars(col(\"text\")))\n",
    "\n",
    "\n",
    "T = T.withColumn('with_url',find_counts(T['url'])).withColumn('with_et_url',find_counts(T['et_url'])\n",
    "                                            ).withColumn('with_display_url',find_counts(T['display_url'])\n",
    "                                            ).withColumn('with_expanded_url',find_counts(T['et_expanded_url'])\n",
    "                                            ).withColumn('hashtag_count',find_counts(T['hashtags'])\n",
    "                                            ).withColumn('et_#_count',find_counts(T['et_hashtags'])\n",
    "                                            ).withColumn(\"with_truncate\", find_truncated(T['truncated']) \n",
    "                                            ).withColumn(\"with_http_tw\", find_http(T['text']) \n",
    "                                            ).withColumn('senti_score',sentiment_analysis_udf('text')\n",
    "                                            ).withColumn('emojis_token',find_counts(emoji('text')) )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+------------------+------------------+---------+--------+------------------+------------------+--------------------+-------------------+------------+\n",
      "|weeks|           song_name|   artist_name|             url_avg|          et_url_avg|     display_url_avg|         hashtag_avg|      et_hashtag_avg|     avg_followers|       avg_friends|        avg_listed|sum_trunc|sum_http|        avg_tokens|          avg_char|           avg_senti|    avg_emoji_count|tweets_count|\n",
      "+-----+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+------------------+------------------+---------+--------+------------------+------------------+--------------------+-------------------+------------+\n",
      "|  5.0|                  Ew| Bryson Tiller|0.037082818294190356|0.006180469715698393|0.037082818294190356|0.030902348578491966|0.054388133498145856|3818.1779975278123| 720.1038318912238|15.233621755253399|       18|     466|19.207663782447465|116.76761433868974|  -0.412391836779153|  0.069221260815822|         809|\n",
      "|  5.0|                  GO|    Marshmello| 0.05194805194805195|0.032467532467532464| 0.05194805194805195|  0.9642857142857143| 0.07467532467532467|2424.3084415584417|1008.1980519480519|14.801948051948052|       13|      19|21.516233766233768|136.69805194805195|  0.6329535828758176|  1.594155844155844|         308|\n",
      "|  1.0|                null| Fleetwood Mac| 0.14655716993051168|0.025900189513581806| 0.14655716993051168| 0.06506632975363234|0.029058749210360075|1634.7384712571068|1083.8559696778268|19.979153506001264|       94|     802|14.161086544535692| 91.29879974731523| 0.27725318595249293|0.17182564750473783|        1583|\n",
      "|  4.0|                null|   Chris Brown| 0.08866257301183166|0.015276321701362887| 0.08866257301183166| 0.05346712595477011| 0.03579451849633069| 1671.597573760671|1016.8948629624083| 7.196046128500824|      321|    1567|17.093305376666166| 96.58634117118466|-0.23668053924638605| 0.1756776995656732|        6677|\n",
      "|  1.0|                null|    Luke Combs| 0.12423625254582485|0.016293279022403257| 0.12423625254582485| 0.07739307535641547| 0.09368635437881874|2618.6048879837067|  538.959266802444|  9.84521384928717|       29|     197| 15.59674134419552| 88.77800407331975| 0.15428452231672776|0.19144602851323828|         491|\n",
      "|  4.0|                null|Paul McCartney| 0.26605504587155965|  0.0563564875491481| 0.26605504587155965| 0.22018348623853212| 0.09174311926605505|2120.3643512450853|1832.6697247706422|32.040629095674966|       74|     458|16.595019659239842|103.23722149410223| 0.11486159825787866| 0.1258191349934469|         763|\n",
      "|  4.0|                Stay|      Rod Wave|                 2.0|                 2.0|                 2.0|                 0.0|                 0.0|            3505.0|            2224.0|               7.0|        1|       1|              22.0|             119.0|  0.8270999789237976|                0.0|           1|\n",
      "|  4.0|              Better|           BTS|              0.0375|0.004166666666666667|              0.0375|               0.025|0.004166666666666667| 801.1354166666666| 791.7229166666667| 6.772916666666666|       16|      23|           20.6125|          117.1875|  0.3047908358693045|0.04791666666666667|         480|\n",
      "|  7.0|                  GO|     BLACKPINK| 0.08524069555787925|0.003408078695635...| 0.08524069555787925|   2.519693272917393| 0.21753611401572365| 590.6041981333024| 516.4100151039851| 2.642655203129236|     1803|    5668| 17.87858719646799|122.31284613299252|  0.1387503601808703|0.25734866968746367|       25821|\n",
      "|  6.0|                  GO|Kendrick Lamar|               0.125|                 0.0|               0.125|                 0.0|                 0.0|            1114.5|             913.0|             1.625|        1|       3|            19.375|           102.625| 0.16949999891221523|                0.5|           8|\n",
      "|  6.0|                null|Kelly Clarkson|  0.3172043010752688| 0.06317204301075269|  0.3172043010752688| 0.17338709677419356| 0.08333333333333333| 36820.42473118279|1383.4180107526881| 162.8736559139785|       79|     425|17.111559139784948| 101.3736559139785| 0.11208830605090786|0.18951612903225806|         744|\n",
      "|  8.0|              Wonder|          null|  0.2306140744060959|0.047288211564320934|  0.2306140744060959| 0.39578664276109365| 0.16427610936799641| 8006.765351860153| 1508.798072613178|40.836396234872254|      797|    1494| 19.47938144329897| 116.6647243388615|  0.3536688494919755|  0.601299865531152|        4462|\n",
      "|  9.0|                null|    Marshmello|   0.297427652733119|0.027331189710610933|   0.297427652733119| 0.26768488745980706| 0.09003215434083602| 20581.09967845659| 996.2194533762058|44.750803858520904|       86|     763|12.258038585209004| 84.91398713826366| 0.36135739892072843| 0.5289389067524116|        1244|\n",
      "| 12.0|                  Ew|    Marshmello|  0.3333333333333333|                 0.0|  0.3333333333333333|                 1.0|                 0.0|           1487.25|            1048.0|13.166666666666666|        2|       5|             17.75|121.41666666666667|  0.2375666710237662| 1.0833333333333333|          12|\n",
      "| 11.0|                Dior|          null| 0.18140589569160998|0.013605442176870748| 0.18140589569160998| 0.17233560090702948| 0.10430839002267574|1783.1972789115646| 916.0249433106576| 9.310657596371883|       71|     145|19.335600907029477|121.10657596371883| 0.12910839183733305| 0.3242630385487528|         441|\n",
      "|  1.0|                null|   Dean Martin| 0.42962962962962964|0.044444444444444446| 0.42962962962962964| 0.14814814814814814| 0.05185185185185185|1749.3925925925926| 1720.962962962963| 57.57037037037037|       17|      76| 18.16296296296296|109.44444444444444| 0.13721629808898325|0.15555555555555556|         135|\n",
      "|  3.0|                  GO|         Ozuna| 0.15151515151515152|0.030303030303030304| 0.15151515151515152|  0.7575757575757576| 0.09090909090909091|33259.606060606064| 674.8484848484849|20.757575757575758|        1|       5|19.515151515151516|127.87878787878788| 0.13109394027428192| 0.2727272727272727|          33|\n",
      "|  3.0|If the World Was ...|          null| 0.38461538461538464| 0.15384615384615385| 0.38461538461538464|  0.6153846153846154|  0.3076923076923077|1406.3846153846155|1229.8461538461538| 9.538461538461538|        2|       8| 21.53846153846154| 112.6923076923077| 0.14655384421348572| 0.8461538461538461|          13|\n",
      "|  4.0|            Diamonds|     Sam Smith|                 1.0| 0.08333333333333333|                 1.0|  0.3333333333333333|                0.75| 755.8333333333334|             714.0|47.458333333333336|        3|      23|15.583333333333334|            93.625| 0.09609583206474781|              0.125|          24|\n",
      "|  3.0|                null|    THE SCOTTS|  0.2079207920792079| 0.06930693069306931|  0.2079207920792079| 0.09900990099009901|  0.2376237623762376|2567.6930693069307| 737.8613861386139| 30.92079207920792|       16|      52|13.257425742574258| 79.98019801980197|   0.080957427178279|0.27722772277227725|         101|\n",
      "+-----+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+------------------+------------------+---------+--------+------------------+------------------+--------------------+-------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# aggregate\n",
    "df = T.groupby(['weeks','song_name','artist_name']).agg(F.mean('with_url').alias('url_avg'),\n",
    "                                                   F.mean('with_et_url').alias('et_url_avg'),\n",
    "                                                   F.mean('with_display_url').alias('display_url_avg'),\n",
    "                                                   F.mean('hashtag_count').alias('hashtag_avg'),\n",
    "                                                   F.mean('et_#_count').alias('et_hashtag_avg'),\n",
    "                                                   F.mean('followers_count').alias('avg_followers'),\n",
    "                                                   F.mean('friends_count').alias('avg_friends'),\n",
    "                                                   F.mean('listed_count').alias('avg_listed'),\n",
    "                                                   _sum('with_truncate').alias('sum_trunc'),\n",
    "                                                   _sum('with_http_tw').alias('sum_http'),\n",
    "                                                   F.mean('tokens').alias('avg_tokens'), \n",
    "                                                   F.mean('char_count').alias('avg_char'),\n",
    "                                                   F.mean('senti_score').alias('avg_senti'),\n",
    "                                                   F.mean('emojis_token').alias('avg_emoji_count'),\n",
    "                                                   F.count('text').alias('tweets_count') ###\n",
    "                                                   \n",
    "                                                  )\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15284"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.select('avg_emoji_count').sort('avg_emoji_count',ascending=False).show()\n",
    "df.count() #15284"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df.toPandas().to_csv('/home/wusean/spotify_0410.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count unique user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-------------+-------------------+-----+\n",
      "|weeks|song_name|  artist_name|            user_id|count|\n",
      "+-----+---------+-------------+-------------------+-----+\n",
      "| 12.0|     null| Taylor Swift| 985916593065476096| 1548|\n",
      "| 12.0|       GO|         null|           24566116| 1358|\n",
      "|  0.0|       GO|         null|           74580436| 1336|\n",
      "| 11.0|       GO|         null|           24566116| 1296|\n",
      "|  2.0|     null|          BTS|1069692236982415361| 1142|\n",
      "| 12.0|     null|          NAV|            7532872|  924|\n",
      "|  6.0|     null|       Future|1123949703232724993|  894|\n",
      "| 10.0|     null|Justin Bieber|         3167508591|  869|\n",
      "|  2.0|     null|          NAV|1238467680773894146|  813|\n",
      "| 10.0|     null|Justin Bieber|1321709362583392257|  806|\n",
      "+-----+---------+-------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user = ds.groupby(['weeks','song_name','artist_name','user_id']).count().sort('count',ascending=False)\n",
    "ds.groupby(['weeks','song_name','artist_name','user_id']).count().sort('count',ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "user.toPandas().to_csv('/home/wusean/spotify_unique_user_0410.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# if-idf word counts (including stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-----------+--------------------+------+----------+--------------------+\n",
      "|weeks|song_name|artist_name|               words|tokens|char_count|            filtered|\n",
      "+-----+---------+-----------+--------------------+------+----------+--------------------+\n",
      "|  0.0|     null|        BTS|[alright, bts, le...|     7|        29|[alright, bts, le...|\n",
      "|  0.0|     null|        BTS|[best, pop, i, m,...|     8|        44|[best, pop, m, re...|\n",
      "|  0.0|     null|        BTS|[rt, mtv, another...|    21|       118|[rt, mtv, another...|\n",
      "|  0.0|     null|        BTS|[bts, doesn, t, k...|    18|        90|[bts, doesn, know...|\n",
      "|  0.0|     null|        BTS|[rt, ebonieseok, ...|    23|       119|[rt, ebonieseok, ...|\n",
      "|  0.0|     null|        BTS|[rt, mtv, another...|    21|       118|[rt, mtv, another...|\n",
      "|  0.0|     null|        BTS|    [best, pop, bts]|     3|        12|    [best, pop, bts]|\n",
      "|  0.0|     null|        SZA|[i, have, joined,...|    10|        54|[joined, black, h...|\n",
      "|  0.0|     null|        BTS|[rt, seokjinfile,...|    17|       106|[rt, seokjinfile,...|\n",
      "|  0.0|     null|  Lady Gaga|[lady, gaga, is, ...|    24|       128|[lady, gaga, sing...|\n",
      "|  0.0|     null|        BTS|[best, pop, bitch...|     6|        46|[best, pop, bitch...|\n",
      "|  0.0|     null|        BTS|[best, pop, btsat...|     3|        19|[best, pop, btsat...|\n",
      "|  0.0|     null|        BTS|[rt, bts_twt, tea...|     7|        43|[rt, bts_twt, tea...|\n",
      "|  0.0|     null|        BTS|[youtube, 30, 08,...|    20|       129|[youtube, 30, 08,...|\n",
      "|  0.0|     null|        BTS|[rt, jinmoonphany...|    21|       140|[rt, jinmoonphany...|\n",
      "|  0.0|     null|        BTS|[shining, through...|    20|       128|[shining, city, l...|\n",
      "|  0.0|     null|        BTS|           [bts, on]|     2|        12|               [bts]|\n",
      "|  0.0|     null|        BTS|           [bts, on]|     2|         6|               [bts]|\n",
      "|  0.0|      Run|       null|[rt, brunoxmusic,...|    10|        63|[rt, brunoxmusic,...|\n",
      "|  0.0|     null|        BTS|[i, don, t, like,...|    27|       140|[like, hylt, play...|\n",
      "+-----+---------+-----------+--------------------+------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tokenize\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# alternatively, pattern=\"\\\\w+\", gaps(False)\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "countChars = udf(lambda text: len(text), IntegerType())\n",
    "\n",
    "## Regextokenize by regex\n",
    "regexTokenized = regexTokenizer.transform(dfbins_en)\n",
    "df_words = regexTokenized.select(\"weeks\",\"song_name\",\"artist_name\",\"text\", \"words\") \\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\"))).withColumn(\"char_count\",countChars(col(\"text\")))\n",
    "\n",
    "# tokens are word counts\n",
    "# count characters in text, including spaces\n",
    "\n",
    "## remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "df_words = remover.transform(df_words)\n",
    "\n",
    "df_words.select('weeks',\"song_name\",\"artist_name\", \"words\",'tokens','char_count','filtered').show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+-------------+-----------------+------------------+\n",
      "|weeks|    song_name|  artist_name|      avg(tokens)|   avg(char_count)|\n",
      "+-----+-------------+-------------+-----------------+------------------+\n",
      "|  1.0|          Run|  Miley Cyrus|             33.0|             136.0|\n",
      "|  8.0|         Stay| Mariah Carey|             33.0|             144.0|\n",
      "|  3.0|           GO|   THE SCOTTS|             33.0|             158.0|\n",
      "|  2.0|           GO|   THE SCOTTS|32.16230366492147|155.38743455497382|\n",
      "|  2.0|         Flex|       Clairo|             32.0|             138.0|\n",
      "| 11.0| Be Like That|       Future|             32.0|             142.0|\n",
      "| 12.0|         Holy|    SAINt JHN|             32.0|             126.0|\n",
      "|  8.0|       Wonder|    Sam Smith|             31.0|             140.0|\n",
      "|  4.0|       lovely|Morgan Wallen|             31.0|             140.0|\n",
      "|  6.0|    That's It|  Miley Cyrus|             31.0|             140.0|\n",
      "|  8.0|    death bed|         null|             31.0|             140.0|\n",
      "|  6.0|    my future|       Clairo|             31.0|             139.0|\n",
      "|  8.0|    Sunflower|        Topic|             31.0|             140.0|\n",
      "|  2.0|         Body|Bryson Tiller|             31.0|             146.0|\n",
      "| 11.0|       Better|       G-Eazy|             31.0|             140.0|\n",
      "| 12.0|        the 1|Justin Bieber|             31.0|             144.0|\n",
      "|  1.0|           GO|   Tate McRae|             31.0|             144.0|\n",
      "|  1.0|           GO|   THE SCOTTS|30.45945945945946|146.40540540540542|\n",
      "|  6.0|     Your Man|         ZAYN|             30.0|             140.0|\n",
      "| 11.0|Electric Love|        BØRNS|             30.0|             140.0|\n",
      "+-----+-------------+-------------+-----------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# average tweet length\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "avg_words = df_words.groupBy(['weeks','song_name','artist_name']).agg(F.mean('tokens'), F.mean('char_count')\n",
    "                                                                ).sort('avg(tokens)',ascending=False)\n",
    "avg_words.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# group weeks, song, artist and combine words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list\n",
    "\n",
    "# grouped_df = df_words.groupby(['weeks','song_name','artist_name']\n",
    "#                           ).agg(collect_list('filtered').alias(\"grouped_filtered\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_df.select('weeks',\"song_name\",\"artist_name\", 'grouped_filtered').show(truncate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import concat_ws\n",
    "\n",
    "# temp = df_words.select('weeks',\"song_name\",\"artist_name\", \"words\",'tokens','char_count','filtered')#.show(truncate=True)\n",
    "# temp.withColumn(\"grouped_filtered\", concat_ws(\", \", \"filtered\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "\n",
    "# T = temp.withColumn(\"explode_word\",explode('filtered'))\n",
    "# T.select('weeks',\"song_name\",\"artist_name\", 'filtered','explode_word').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list\n",
    "\n",
    "# group_df = T.groupby(['weeks','song_name','artist_name']\n",
    "#                           ).agg(collect_list('explode_word').alias(\"group_wd\"))\n",
    "\n",
    "# group = group_df.select('weeks',\"song_name\",\"artist_name\",\"group_wd\")\n",
    "# group.filter(col('weeks')==9).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_wd = udf(lambda line: len(line), IntegerType())\n",
    "\n",
    "# group.select(\"weeks\",\"song_name\",\"artist_name\", count_wd(col(\"group_wd\"))).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TD-IDF\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(df_words)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-----------+--------------------+\n",
      "|weeks|song_name|artist_name|            features|\n",
      "+-----+---------+-----------+--------------------+\n",
      "|  0.0|     null|        BTS|(20,[14,15,17,19]...|\n",
      "|  0.0|     null|        BTS|(20,[2,3,4,5,14,1...|\n",
      "|  0.0|     null|        BTS|(20,[3,5,7,8,13,1...|\n",
      "|  0.0|     null|        BTS|(20,[0,6,13,14,15...|\n",
      "|  0.0|     null|        BTS|(20,[0,4,10,13,14...|\n",
      "|  0.0|     null|        BTS|(20,[3,5,7,8,13,1...|\n",
      "|  0.0|     null|        BTS|(20,[3,14,19],[0....|\n",
      "|  0.0|     null|        SZA|(20,[4,9,10,11,18...|\n",
      "|  0.0|     null|        BTS|(20,[2,6,9,11,12,...|\n",
      "|  0.0|     null|  Lady Gaga|(20,[1,4,5,6,11,1...|\n",
      "|  0.0|     null|        BTS|(20,[3,17,18,19],...|\n",
      "|  0.0|     null|        BTS|(20,[2,3,19],[0.9...|\n",
      "|  0.0|     null|        BTS|(20,[4,5,7,11,13]...|\n",
      "|  0.0|     null|        BTS|(20,[1,2,3,5,6,9,...|\n",
      "|  0.0|     null|        BTS|(20,[2,3,4,6,8,10...|\n",
      "|  0.0|     null|        BTS|(20,[0,1,6,9,10,1...|\n",
      "|  0.0|     null|        BTS|(20,[14],[0.60234...|\n",
      "|  0.0|     null|        BTS|(20,[14],[0.60234...|\n",
      "|  0.0|      Run|       null|(20,[3,5,8,9,10,1...|\n",
      "|  0.0|     null|        BTS|(20,[1,5,8,10,14,...|\n",
      "+-----+---------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescaledData.select('weeks','song_name','artist_name','features').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TD-IDF\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "# group = group.filter(col('weeks')==9)\n",
    "group = group_df.select('weeks',\"song_name\",\"artist_name\",\"group_wd\")\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"group_wd\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(group)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+-------------------+--------------------+--------------------+--------------------+\n",
      "|weeks|        song_name|        artist_name|            group_wd|         rawFeatures|            features|\n",
      "+-----+-----------------+-------------------+--------------------+--------------------+--------------------+\n",
      "|  9.0|             null|         Marshmello|[liked, spotify, ...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|\n",
      "|  9.0|           Slidin|               null|[rt, colesoulpodc...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|\n",
      "|  9.0|        Sunflower|       Harry Styles|[rt, thehscharts,...|(20,[2,5,10,11,13...|(20,[2,5,10,11,13...|\n",
      "|  9.0|             null|          Burl Ives|[rt, southern_liv...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|\n",
      "|  9.0|              Run|         Marshmello|[marshmellomusic,...|(20,[1,2,3,4,5,7,...|(20,[1,2,3,4,5,7,...|\n",
      "|  9.0|           Better|                DDG|[baddgrl_, harkso...|(20,[1,2,4,10,11,...|(20,[1,2,4,10,11,...|\n",
      "|  9.0|      WITHOUT YOU|                 CJ|[daily, ava, max,...|(20,[3,4,9,10,11,...|(20,[3,4,9,10,11,...|\n",
      "|  9.0|        positions|               null|[rt, joangrande, ...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|\n",
      "|  9.0|        Sunflower|              Wham!|[ashemusic, jsunf...|(20,[2,3,6,9,15],...|(20,[2,3,6,9,15],...|\n",
      "|  9.0|            the 1|  The Neighbourhood|[blckstair, d, pr...|(20,[2,3,4,10,13,...|(20,[2,3,4,10,13,...|\n",
      "|  9.0|               Ew|      Ty Dolla $ign|[new, music, made...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|\n",
      "|  9.0|             null|           King Von|[king, von, got, ...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|\n",
      "|  9.0|             Holy|                 CJ|[wizkid, started,...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|\n",
      "|  9.0|               GO|Megan Thee Stallion|[megan, thee, sta...|(20,[0,1,3,4,5,6,...|(20,[0,1,3,4,5,6,...|\n",
      "|  9.0|             Body|              Gunna|[somebody, get, h...|(20,[0,2,6,9,10,1...|(20,[0,2,6,9,10,1...|\n",
      "|  9.0|          Falling|           Rod Wave|[talm, bout, rod,...|(20,[0,2,5,7,8,9,...|(20,[0,2,5,7,8,9,...|\n",
      "|  9.0|        positions|                DDG|[haven, listen, f...|(20,[3,4,5,7,8,10...|(20,[3,4,5,7,8,10...|\n",
      "|  9.0|       Excitement|        Chris Brown|[trippieredd, moo...|(20,[0,3,4,8,12,1...|(20,[0,3,4,8,12,1...|\n",
      "|  9.0|               GO|                BTS|[rt, btsvotingorg...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|\n",
      "|  9.0|How You Like That|                BTS|[rt, kchartsmaste...|(20,[1,2,3,4,6,7,...|(20,[1,2,3,4,6,7,...|\n",
      "+-----+-----------------+-------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescaledData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# S = rescaledData.toPandas()\n",
    "# S.to_csv('/home/wusean/spotify_tfidf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.write.csv('mycsv.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"vec_features\")\n",
    "model = cv.fit(df_words)\n",
    "countVectorizer_feateures = model.transform(df_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-----------+--------------------+\n",
      "|weeks|song_name|artist_name|            features|\n",
      "+-----+---------+-----------+--------------------+\n",
      "|  0.0|     null|        BTS|(262144,[4,42,156...|\n",
      "|  0.0|     null|        BTS|(262144,[3,4,9,32...|\n",
      "|  0.0|     null|        BTS|(262144,[0,1,2,3,...|\n",
      "|  0.0|     null|        BTS|(262144,[1,2,4,55...|\n",
      "|  0.0|     null|        BTS|(262144,[0,4,101,...|\n",
      "|  0.0|     null|        BTS|(262144,[0,1,2,3,...|\n",
      "|  0.0|     null|        BTS|(262144,[4,32,37]...|\n",
      "|  0.0|     null|        SZA|(262144,[1,2,272,...|\n",
      "|  0.0|     null|        BTS|(262144,[0,1,2,18...|\n",
      "|  0.0|     null|  Lady Gaga|(262144,[110,342,...|\n",
      "|  0.0|     null|        BTS|(262144,[32,37,72...|\n",
      "|  0.0|     null|        BTS|(262144,[32,37,54...|\n",
      "|  0.0|     null|        BTS|(262144,[0,3,177,...|\n",
      "|  0.0|     null|        BTS|(262144,[1,2,3,4,...|\n",
      "|  0.0|     null|        BTS|(262144,[0,128,29...|\n",
      "|  0.0|     null|        BTS|(262144,[1,2,23,1...|\n",
      "|  0.0|     null|        BTS|  (262144,[4],[1.0])|\n",
      "|  0.0|     null|        BTS|  (262144,[4],[1.0])|\n",
      "|  0.0|      Run|       null|(262144,[0,1,2,18...|\n",
      "|  0.0|     null|        BTS|(262144,[1,2,4,16...|\n",
      "+-----+---------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countVectorizer_feateures.select('weeks','song_name','artist_name','vec_features').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o702.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 143 in stage 40.0 failed 4 times, most recent failure: Lost task 143.3 in stage 40.0 (TID 20045, cavium-dn0021.arc-ts.umich.edu, executor 186): ExecutorLostFailure (executor 186 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 121438 ms\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1158)\n\tat org.apache.spark.ml.feature.CountVectorizer.fit(CountVectorizer.scala:176)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-527e709aa685>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"group_wd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"vec_features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mcountVectorizer_feateures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \"\"\"\n\u001b[1;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o702.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 143 in stage 40.0 failed 4 times, most recent failure: Lost task 143.3 in stage 40.0 (TID 20045, cavium-dn0021.arc-ts.umich.edu, executor 186): ExecutorLostFailure (executor 186 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 121438 ms\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1158)\n\tat org.apache.spark.ml.feature.CountVectorizer.fit(CountVectorizer.scala:176)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"group_wd\", outputCol=\"vec_features\")\n",
    "model = cv.fit(group)\n",
    "countVectorizer_feateures = model.transform(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVectorizer_feateures.select('weeks','song_name','artist_name','vec_features').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-idf\n",
    "# https://spark.apache.org/docs/2.2.0/ml-features.html#tf-idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using Vader for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vader sentiment calculation\n",
    "## ref: https://github.com/cjhutto/vaderSentiment\n",
    "\n",
    "# coding: utf-8\n",
    "# Author: C.J. Hutto\n",
    "# Thanks to George Berry for reducing the time complexity from something like O(N^4) to O(N).\n",
    "# Thanks to Ewan Klein and Pierpaolo Pantone for bringing VADER into NLTK. Those modifications were awesome.\n",
    "# For license information, see LICENSE.TXT\n",
    "\n",
    "\"\"\"\n",
    "If you use the VADER sentiment analysis tools, please cite:\n",
    "Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for\n",
    "Sentiment Analysis of Social Media Text. Eighth International Conference on\n",
    "Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.\n",
    "\"\"\"\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "import codecs\n",
    "import json\n",
    "from itertools import product\n",
    "from inspect import getsourcefile\n",
    "from io import open\n",
    "\n",
    "# ##Constants##\n",
    "\n",
    "# (empirically derived mean sentiment intensity rating increase for booster words)\n",
    "B_INCR = 0.293\n",
    "B_DECR = -0.293\n",
    "\n",
    "# (empirically derived mean sentiment intensity rating increase for using ALLCAPs to emphasize a word)\n",
    "C_INCR = 0.733\n",
    "N_SCALAR = -0.74\n",
    "\n",
    "NEGATE = \\\n",
    "    [\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\",\n",
    "     \"ain't\", \"aren't\", \"can't\", \"couldn't\", \"daren't\", \"didn't\", \"doesn't\",\n",
    "     \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\", \"neither\",\n",
    "     \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n",
    "     \"neednt\", \"needn't\", \"never\", \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\",\n",
    "     \"oughtnt\", \"shant\", \"shouldnt\", \"uhuh\", \"wasnt\", \"werent\",\n",
    "     \"oughtn't\", \"shan't\", \"shouldn't\", \"uh-uh\", \"wasn't\", \"weren't\",\n",
    "     \"without\", \"wont\", \"wouldnt\", \"won't\", \"wouldn't\", \"rarely\", \"seldom\", \"despite\"]\n",
    "\n",
    "# booster/dampener 'intensifiers' or 'degree adverbs'\n",
    "# http://en.wiktionary.org/wiki/Category:English_degree_adverbs\n",
    "\n",
    "BOOSTER_DICT = \\\n",
    "    {\"absolutely\": B_INCR, \"amazingly\": B_INCR, \"awfully\": B_INCR, \n",
    "     \"completely\": B_INCR, \"considerable\": B_INCR, \"considerably\": B_INCR,\n",
    "     \"decidedly\": B_INCR, \"deeply\": B_INCR, \"effing\": B_INCR, \"enormous\": B_INCR, \"enormously\": B_INCR,\n",
    "     \"entirely\": B_INCR, \"especially\": B_INCR, \"exceptional\": B_INCR, \"exceptionally\": B_INCR, \n",
    "     \"extreme\": B_INCR, \"extremely\": B_INCR,\n",
    "     \"fabulously\": B_INCR, \"flipping\": B_INCR, \"flippin\": B_INCR, \"frackin\": B_INCR, \"fracking\": B_INCR,\n",
    "     \"fricking\": B_INCR, \"frickin\": B_INCR, \"frigging\": B_INCR, \"friggin\": B_INCR, \"fully\": B_INCR, \n",
    "     \"fuckin\": B_INCR, \"fucking\": B_INCR, \"fuggin\": B_INCR, \"fugging\": B_INCR,\n",
    "     \"greatly\": B_INCR, \"hella\": B_INCR, \"highly\": B_INCR, \"hugely\": B_INCR, \n",
    "     \"incredible\": B_INCR, \"incredibly\": B_INCR, \"intensely\": B_INCR, \n",
    "     \"major\": B_INCR, \"majorly\": B_INCR, \"more\": B_INCR, \"most\": B_INCR, \"particularly\": B_INCR,\n",
    "     \"purely\": B_INCR, \"quite\": B_INCR, \"really\": B_INCR, \"remarkably\": B_INCR,\n",
    "     \"so\": B_INCR, \"substantially\": B_INCR,\n",
    "     \"thoroughly\": B_INCR, \"total\": B_INCR, \"totally\": B_INCR, \"tremendous\": B_INCR, \"tremendously\": B_INCR,\n",
    "     \"uber\": B_INCR, \"unbelievably\": B_INCR, \"unusually\": B_INCR, \"utter\": B_INCR, \"utterly\": B_INCR,\n",
    "     \"very\": B_INCR,\n",
    "     \"almost\": B_DECR, \"barely\": B_DECR, \"hardly\": B_DECR, \"just enough\": B_DECR,\n",
    "     \"kind of\": B_DECR, \"kinda\": B_DECR, \"kindof\": B_DECR, \"kind-of\": B_DECR,\n",
    "     \"less\": B_DECR, \"little\": B_DECR, \"marginal\": B_DECR, \"marginally\": B_DECR,\n",
    "     \"occasional\": B_DECR, \"occasionally\": B_DECR, \"partly\": B_DECR,\n",
    "     \"scarce\": B_DECR, \"scarcely\": B_DECR, \"slight\": B_DECR, \"slightly\": B_DECR, \"somewhat\": B_DECR,\n",
    "     \"sort of\": B_DECR, \"sorta\": B_DECR, \"sortof\": B_DECR, \"sort-of\": B_DECR}\n",
    "\n",
    "# check for sentiment laden idioms that do not contain lexicon words (future work, not yet implemented)\n",
    "SENTIMENT_LADEN_IDIOMS = {\"cut the mustard\": 2, \"hand to mouth\": -2,\n",
    "                          \"back handed\": -2, \"blow smoke\": -2, \"blowing smoke\": -2,\n",
    "                          \"upper hand\": 1, \"break a leg\": 2,\n",
    "                          \"cooking with gas\": 2, \"in the black\": 2, \"in the red\": -2,\n",
    "                          \"on the ball\": 2, \"under the weather\": -2}\n",
    "\n",
    "# check for special case idioms and phrases containing lexicon words\n",
    "SPECIAL_CASES = {\"the shit\": 3, \"the bomb\": 3, \"bad ass\": 1.5, \"badass\": 1.5, \"bus stop\": 0.0,\n",
    "                 \"yeah right\": -2, \"kiss of death\": -1.5, \"to die for\": 3, \n",
    "                 \"beating heart\": 3.1, \"broken heart\": -2.9 }\n",
    "\n",
    "\n",
    "# #Static methods# #\n",
    "\n",
    "def negated(input_words, include_nt=True):\n",
    "    \"\"\"\n",
    "    Determine if input contains negation words\n",
    "    \"\"\"\n",
    "    input_words = [str(w).lower() for w in input_words]\n",
    "    neg_words = []\n",
    "    neg_words.extend(NEGATE)\n",
    "    for word in neg_words:\n",
    "        if word in input_words:\n",
    "            return True\n",
    "    if include_nt:\n",
    "        for word in input_words:\n",
    "            if \"n't\" in word:\n",
    "                return True\n",
    "    '''if \"least\" in input_words:\n",
    "        i = input_words.index(\"least\")\n",
    "        if i > 0 and input_words[i - 1] != \"at\":\n",
    "            return True'''\n",
    "    return False\n",
    "\n",
    "\n",
    "def normalize(score, alpha=15):\n",
    "    \"\"\"\n",
    "    Normalize the score to be between -1 and 1 using an alpha that\n",
    "    approximates the max expected value\n",
    "    \"\"\"\n",
    "    norm_score = score / math.sqrt((score * score) + alpha)\n",
    "    if norm_score < -1.0:\n",
    "        return -1.0\n",
    "    elif norm_score > 1.0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return norm_score\n",
    "\n",
    "\n",
    "def allcap_differential(words):\n",
    "    \"\"\"\n",
    "    Check whether just some words in the input are ALL CAPS\n",
    "    :param list words: The words to inspect\n",
    "    :returns: `True` if some but not all items in `words` are ALL CAPS\n",
    "    \"\"\"\n",
    "    is_different = False\n",
    "    allcap_words = 0\n",
    "    for word in words:\n",
    "        if word.isupper():\n",
    "            allcap_words += 1\n",
    "    cap_differential = len(words) - allcap_words\n",
    "    if 0 < cap_differential < len(words):\n",
    "        is_different = True\n",
    "    return is_different\n",
    "\n",
    "\n",
    "def scalar_inc_dec(word, valence, is_cap_diff):\n",
    "    \"\"\"\n",
    "    Check if the preceding words increase, decrease, or negate/nullify the\n",
    "    valence\n",
    "    \"\"\"\n",
    "    scalar = 0.0\n",
    "    word_lower = word.lower()\n",
    "    if word_lower in BOOSTER_DICT:\n",
    "        scalar = BOOSTER_DICT[word_lower]\n",
    "        if valence < 0:\n",
    "            scalar *= -1\n",
    "        # check if booster/dampener word is in ALLCAPS (while others aren't)\n",
    "        if word.isupper() and is_cap_diff:\n",
    "            if valence > 0:\n",
    "                scalar += C_INCR\n",
    "            else:\n",
    "                scalar -= C_INCR\n",
    "    return scalar\n",
    "\n",
    "\n",
    "class SentiText(object):\n",
    "    \"\"\"\n",
    "    Identify sentiment-relevant string-level properties of input text.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text):\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text).encode('utf-8')\n",
    "        self.text = text\n",
    "        self.words_and_emoticons = self._words_and_emoticons()\n",
    "        # doesn't separate words from\\\n",
    "        # adjacent punctuation (keeps emoticons & contractions)\n",
    "        self.is_cap_diff = allcap_differential(self.words_and_emoticons)\n",
    "\n",
    "    @staticmethod\n",
    "    def _strip_punc_if_word(token):\n",
    "        \"\"\"\n",
    "        Removes all trailing and leading punctuation\n",
    "        If the resulting string has two or fewer characters,\n",
    "        then it was likely an emoticon, so return original string\n",
    "        (ie \":)\" stripped would be \"\", so just return \":)\"\n",
    "        \"\"\"\n",
    "        stripped = token.strip(string.punctuation)\n",
    "        if len(stripped) <= 2:\n",
    "            return token\n",
    "        return stripped\n",
    "\n",
    "    def _words_and_emoticons(self):\n",
    "        \"\"\"\n",
    "        Removes leading and trailing puncutation\n",
    "        Leaves contractions and most emoticons\n",
    "            Does not preserve punc-plus-letter emoticons (e.g. :D)\n",
    "        \"\"\"\n",
    "        wes = self.text.split()\n",
    "        stripped = list(map(self._strip_punc_if_word, wes))\n",
    "        return stripped\n",
    "\n",
    "class SentimentIntensityAnalyzer_1(object):\n",
    "    \"\"\"\n",
    "    Give a sentiment intensity score to sentences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lexicon_dict, emoji_dict):\n",
    "        self.lexicon = lexicon_dict\n",
    "        self.emojis = emoji_dict\n",
    "\n",
    "    def polarity_scores(self, text):\n",
    "        \"\"\"\n",
    "        Return a float for sentiment strength based on the input text.\n",
    "        Positive values are positive valence, negative value are negative\n",
    "        valence.\n",
    "        \"\"\"\n",
    "        # convert emojis to their textual descriptions\n",
    "        text_no_emoji = \"\"\n",
    "        prev_space = True\n",
    "        for chr in text:\n",
    "            if chr in self.emojis:\n",
    "                # get the textual description\n",
    "                description = self.emojis[chr]\n",
    "                if not prev_space:\n",
    "                    text_no_emoji += ' '\n",
    "                text_no_emoji += description\n",
    "                prev_space = False\n",
    "            else:\n",
    "                text_no_emoji += chr\n",
    "                prev_space = chr == ' '\n",
    "        text = text_no_emoji.strip()\n",
    "\n",
    "        sentitext = SentiText(text)\n",
    "\n",
    "        sentiments = []\n",
    "        words_and_emoticons = sentitext.words_and_emoticons\n",
    "        for i, item in enumerate(words_and_emoticons):\n",
    "            valence = 0\n",
    "            # check for vader_lexicon words that may be used as modifiers or negations\n",
    "            if item.lower() in BOOSTER_DICT:\n",
    "                sentiments.append(valence)\n",
    "                continue\n",
    "            if (i < len(words_and_emoticons) - 1 and item.lower() == \"kind\" and\n",
    "                    words_and_emoticons[i + 1].lower() == \"of\"):\n",
    "                sentiments.append(valence)\n",
    "                continue\n",
    "\n",
    "            sentiments = self.sentiment_valence(valence, sentitext, item, i, sentiments)\n",
    "\n",
    "        sentiments = self._but_check(words_and_emoticons, sentiments)\n",
    "\n",
    "        valence_dict = self.score_valence(sentiments, text)\n",
    "\n",
    "        return valence_dict\n",
    "\n",
    "    def sentiment_valence(self, valence, sentitext, item, i, sentiments):\n",
    "        is_cap_diff = sentitext.is_cap_diff\n",
    "        words_and_emoticons = sentitext.words_and_emoticons\n",
    "        item_lowercase = item.lower()\n",
    "        if item_lowercase in self.lexicon:\n",
    "            # get the sentiment valence \n",
    "            valence = self.lexicon[item_lowercase]\n",
    "                \n",
    "            # check for \"no\" as negation for an adjacent lexicon item vs \"no\" as its own stand-alone lexicon item\n",
    "            if item_lowercase == \"no\" and i != len(words_and_emoticons)-1 and words_and_emoticons[i + 1].lower() in self.lexicon:\n",
    "                # don't use valence of \"no\" as a lexicon item. Instead set it's valence to 0.0 and negate the next item\n",
    "                valence = 0.0\n",
    "            if (i > 0 and words_and_emoticons[i - 1].lower() == \"no\") \\\n",
    "               or (i > 1 and words_and_emoticons[i - 2].lower() == \"no\") \\\n",
    "               or (i > 2 and words_and_emoticons[i - 3].lower() == \"no\" and words_and_emoticons[i - 1].lower() in [\"or\", \"nor\"] ):\n",
    "                valence = self.lexicon[item_lowercase] * N_SCALAR\n",
    "            \n",
    "            # check if sentiment laden word is in ALL CAPS (while others aren't)\n",
    "            if item.isupper() and is_cap_diff:\n",
    "                if valence > 0:\n",
    "                    valence += C_INCR\n",
    "                else:\n",
    "                    valence -= C_INCR\n",
    "\n",
    "            for start_i in range(0, 3):\n",
    "                # dampen the scalar modifier of preceding words and emoticons\n",
    "                # (excluding the ones that immediately preceed the item) based\n",
    "                # on their distance from the current item.\n",
    "                if i > start_i and words_and_emoticons[i - (start_i + 1)].lower() not in self.lexicon:\n",
    "                    s = scalar_inc_dec(words_and_emoticons[i - (start_i + 1)], valence, is_cap_diff)\n",
    "                    if start_i == 1 and s != 0:\n",
    "                        s = s * 0.95\n",
    "                    if start_i == 2 and s != 0:\n",
    "                        s = s * 0.9\n",
    "                    valence = valence + s\n",
    "                    valence = self._negation_check(valence, words_and_emoticons, start_i, i)\n",
    "                    if start_i == 2:\n",
    "                        valence = self._special_idioms_check(valence, words_and_emoticons, i)\n",
    "\n",
    "            valence = self._least_check(valence, words_and_emoticons, i)\n",
    "        sentiments.append(valence)\n",
    "        return sentiments\n",
    "\n",
    "    def _least_check(self, valence, words_and_emoticons, i):\n",
    "        # check for negation case using \"least\"\n",
    "        if i > 1 and words_and_emoticons[i - 1].lower() not in self.lexicon \\\n",
    "                and words_and_emoticons[i - 1].lower() == \"least\":\n",
    "            if words_and_emoticons[i - 2].lower() != \"at\" and words_and_emoticons[i - 2].lower() != \"very\":\n",
    "                valence = valence * N_SCALAR\n",
    "        elif i > 0 and words_and_emoticons[i - 1].lower() not in self.lexicon \\\n",
    "                and words_and_emoticons[i - 1].lower() == \"least\":\n",
    "            valence = valence * N_SCALAR\n",
    "        return valence\n",
    "\n",
    "    @staticmethod\n",
    "    def _but_check(words_and_emoticons, sentiments):\n",
    "        # check for modification in sentiment due to contrastive conjunction 'but'\n",
    "        words_and_emoticons_lower = [str(w).lower() for w in words_and_emoticons]\n",
    "        if 'but' in words_and_emoticons_lower:\n",
    "            bi = words_and_emoticons_lower.index('but')\n",
    "            for sentiment in sentiments:\n",
    "                si = sentiments.index(sentiment)\n",
    "                if si < bi:\n",
    "                    sentiments.pop(si)\n",
    "                    sentiments.insert(si, sentiment * 0.5)\n",
    "                elif si > bi:\n",
    "                    sentiments.pop(si)\n",
    "                    sentiments.insert(si, sentiment * 1.5)\n",
    "        return sentiments\n",
    "\n",
    "    @staticmethod\n",
    "    def _special_idioms_check(valence, words_and_emoticons, i):\n",
    "        words_and_emoticons_lower = [str(w).lower() for w in words_and_emoticons]\n",
    "        onezero = \"{0} {1}\".format(words_and_emoticons_lower[i - 1], words_and_emoticons_lower[i])\n",
    "\n",
    "        twoonezero = \"{0} {1} {2}\".format(words_and_emoticons_lower[i - 2],\n",
    "                                          words_and_emoticons_lower[i - 1], words_and_emoticons_lower[i])\n",
    "\n",
    "        twoone = \"{0} {1}\".format(words_and_emoticons_lower[i - 2], words_and_emoticons_lower[i - 1])\n",
    "\n",
    "        threetwoone = \"{0} {1} {2}\".format(words_and_emoticons_lower[i - 3],\n",
    "                                           words_and_emoticons_lower[i - 2], words_and_emoticons_lower[i - 1])\n",
    "\n",
    "        threetwo = \"{0} {1}\".format(words_and_emoticons_lower[i - 3], words_and_emoticons_lower[i - 2])\n",
    "\n",
    "        sequences = [onezero, twoonezero, twoone, threetwoone, threetwo]\n",
    "\n",
    "        for seq in sequences:\n",
    "            if seq in SPECIAL_CASES:\n",
    "                valence = SPECIAL_CASES[seq]\n",
    "                break\n",
    "\n",
    "        if len(words_and_emoticons_lower) - 1 > i:\n",
    "            zeroone = \"{0} {1}\".format(words_and_emoticons_lower[i], words_and_emoticons_lower[i + 1])\n",
    "            if zeroone in SPECIAL_CASES:\n",
    "                valence = SPECIAL_CASES[zeroone]\n",
    "        if len(words_and_emoticons_lower) - 1 > i + 1:\n",
    "            zeroonetwo = \"{0} {1} {2}\".format(words_and_emoticons_lower[i], words_and_emoticons_lower[i + 1],\n",
    "                                              words_and_emoticons_lower[i + 2])\n",
    "            if zeroonetwo in SPECIAL_CASES:\n",
    "                valence = SPECIAL_CASES[zeroonetwo]\n",
    "\n",
    "        # check for booster/dampener bi-grams such as 'sort of' or 'kind of'\n",
    "        n_grams = [threetwoone, threetwo, twoone]\n",
    "        for n_gram in n_grams:\n",
    "            if n_gram in BOOSTER_DICT:\n",
    "                valence = valence + BOOSTER_DICT[n_gram]\n",
    "        return valence\n",
    "\n",
    "    @staticmethod\n",
    "    def _sentiment_laden_idioms_check(valence, senti_text_lower):\n",
    "        # Future Work\n",
    "        # check for sentiment laden idioms that don't contain a lexicon word\n",
    "        idioms_valences = []\n",
    "        for idiom in SENTIMENT_LADEN_IDIOMS:\n",
    "            if idiom in senti_text_lower:\n",
    "                print(idiom, senti_text_lower)\n",
    "                valence = SENTIMENT_LADEN_IDIOMS[idiom]\n",
    "                idioms_valences.append(valence)\n",
    "        if len(idioms_valences) > 0:\n",
    "            valence = sum(idioms_valences) / float(len(idioms_valences))\n",
    "        return valence\n",
    "\n",
    "    @staticmethod\n",
    "    def _negation_check(valence, words_and_emoticons, start_i, i):\n",
    "        words_and_emoticons_lower = [str(w).lower() for w in words_and_emoticons]\n",
    "        if start_i == 0:\n",
    "            if negated([words_and_emoticons_lower[i - (start_i + 1)]]):  # 1 word preceding lexicon word (w/o stopwords)\n",
    "                valence = valence * N_SCALAR\n",
    "        if start_i == 1:\n",
    "            if words_and_emoticons_lower[i - 2] == \"never\" and \\\n",
    "                    (words_and_emoticons_lower[i - 1] == \"so\" or\n",
    "                     words_and_emoticons_lower[i - 1] == \"this\"):\n",
    "                valence = valence * 1.25\n",
    "            elif words_and_emoticons_lower[i - 2] == \"without\" and \\\n",
    "                    words_and_emoticons_lower[i - 1] == \"doubt\":\n",
    "                valence = valence\n",
    "            elif negated([words_and_emoticons_lower[i - (start_i + 1)]]):  # 2 words preceding the lexicon word position\n",
    "                valence = valence * N_SCALAR\n",
    "        if start_i == 2:\n",
    "            if words_and_emoticons_lower[i - 3] == \"never\" and \\\n",
    "                    (words_and_emoticons_lower[i - 2] == \"so\" or words_and_emoticons_lower[i - 2] == \"this\") or \\\n",
    "                    (words_and_emoticons_lower[i - 1] == \"so\" or words_and_emoticons_lower[i - 1] == \"this\"):\n",
    "                valence = valence * 1.25\n",
    "            elif words_and_emoticons_lower[i - 3] == \"without\" and \\\n",
    "                    (words_and_emoticons_lower[i - 2] == \"doubt\" or words_and_emoticons_lower[i - 1] == \"doubt\"):\n",
    "                valence = valence\n",
    "            elif negated([words_and_emoticons_lower[i - (start_i + 1)]]):  # 3 words preceding the lexicon word position\n",
    "                valence = valence * N_SCALAR\n",
    "        return valence\n",
    "\n",
    "    def _punctuation_emphasis(self, text):\n",
    "        # add emphasis from exclamation points and question marks\n",
    "        ep_amplifier = self._amplify_ep(text)\n",
    "        qm_amplifier = self._amplify_qm(text)\n",
    "        punct_emph_amplifier = ep_amplifier + qm_amplifier\n",
    "        return punct_emph_amplifier\n",
    "\n",
    "    @staticmethod\n",
    "    def _amplify_ep(text):\n",
    "        # check for added emphasis resulting from exclamation points (up to 4 of them)\n",
    "        ep_count = text.count(\"!\")\n",
    "        if ep_count > 4:\n",
    "            ep_count = 4\n",
    "        # (empirically derived mean sentiment intensity rating increase for\n",
    "        # exclamation points)\n",
    "        ep_amplifier = ep_count * 0.292\n",
    "        return ep_amplifier\n",
    "\n",
    "    @staticmethod\n",
    "    def _amplify_qm(text):\n",
    "        # check for added emphasis resulting from question marks (2 or 3+)\n",
    "        qm_count = text.count(\"?\")\n",
    "        qm_amplifier = 0\n",
    "        if qm_count > 1:\n",
    "            if qm_count <= 3:\n",
    "                # (empirically derived mean sentiment intensity rating increase for\n",
    "                # question marks)\n",
    "                qm_amplifier = qm_count * 0.18\n",
    "            else:\n",
    "                qm_amplifier = 0.96\n",
    "        return qm_amplifier\n",
    "\n",
    "    @staticmethod\n",
    "    def _sift_sentiment_scores(sentiments):\n",
    "        # want separate positive versus negative sentiment scores\n",
    "        pos_sum = 0.0\n",
    "        neg_sum = 0.0\n",
    "        neu_count = 0\n",
    "        for sentiment_score in sentiments:\n",
    "            if sentiment_score > 0:\n",
    "                pos_sum += (float(sentiment_score) + 1)  # compensates for neutral words that are counted as 1\n",
    "            if sentiment_score < 0:\n",
    "                neg_sum += (float(sentiment_score) - 1)  # when used with math.fabs(), compensates for neutrals\n",
    "            if sentiment_score == 0:\n",
    "                neu_count += 1\n",
    "        return pos_sum, neg_sum, neu_count\n",
    "\n",
    "    def score_valence(self, sentiments, text):\n",
    "        if sentiments:\n",
    "            sum_s = float(sum(sentiments))\n",
    "            # compute and add emphasis from punctuation in text\n",
    "            punct_emph_amplifier = self._punctuation_emphasis(text)\n",
    "            if sum_s > 0:\n",
    "                sum_s += punct_emph_amplifier\n",
    "            elif sum_s < 0:\n",
    "                sum_s -= punct_emph_amplifier\n",
    "\n",
    "            compound = normalize(sum_s)\n",
    "            # discriminate between positive, negative and neutral sentiment scores\n",
    "            pos_sum, neg_sum, neu_count = self._sift_sentiment_scores(sentiments)\n",
    "\n",
    "            if pos_sum > math.fabs(neg_sum):\n",
    "                pos_sum += punct_emph_amplifier\n",
    "            elif pos_sum < math.fabs(neg_sum):\n",
    "                neg_sum -= punct_emph_amplifier\n",
    "\n",
    "            total = pos_sum + math.fabs(neg_sum) + neu_count\n",
    "            pos = math.fabs(pos_sum / total)\n",
    "            neg = math.fabs(neg_sum / total)\n",
    "            neu = math.fabs(neu_count / total)\n",
    "\n",
    "        else:\n",
    "            compound = 0.0\n",
    "            pos = 0.0\n",
    "            neg = 0.0\n",
    "            neu = 0.0\n",
    "\n",
    "        sentiment_dict = \\\n",
    "            {\"neg\": round(neg, 3),\n",
    "             \"neu\": round(neu, 3),\n",
    "             \"pos\": round(pos, 3),\n",
    "             \"compound\": round(compound, 4)}\n",
    "\n",
    "        return sentiment_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dictionary for sentiment \n",
    "\n",
    "lexicon_file=\"vader_lexicon.txt\"\n",
    "emoji_lexicon=\"emoji_utf8_lexicon.txt\"\n",
    "\n",
    "\n",
    "def make_lex_dict(lexicon_full_filepath):\n",
    "    \"\"\"\n",
    "    Convert lexicon file to a dictionary\n",
    "    \"\"\"\n",
    "    lex_dict = {}\n",
    "    for line in lexicon_full_filepath.rstrip('\\n').split('\\n'):\n",
    "        if not line:\n",
    "            continue\n",
    "        (word, measure) = line.strip().split('\\t')[0:2]\n",
    "        lex_dict[word] = float(measure)\n",
    "    return lex_dict\n",
    "\n",
    "def make_emoji_dict(emoji_full_filepath):\n",
    "    \"\"\"\n",
    "    Convert emoji lexicon file to a dictionary\n",
    "    \"\"\"\n",
    "    emoji_dict = {}\n",
    "    for line in emoji_full_filepath.rstrip('\\n').split('\\n'):\n",
    "        (emoji, description) = line.strip().split('\\t')[0:2]\n",
    "        emoji_dict[emoji] = description\n",
    "    return emoji_dict\n",
    "\n",
    "with codecs.open(lexicon_file, encoding='utf-8') as f:\n",
    "    lexicon_full_filepath = f.read()\n",
    "    \n",
    "lexicon_dict = make_lex_dict(lexicon_full_filepath)\n",
    "\n",
    "with codecs.open(emoji_lexicon, encoding='utf-8') as f:\n",
    "    emoji_full_filepath = f.read()\n",
    "emojis_dict = make_emoji_dict(emoji_full_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "# from textblob import TextBlob\n",
    "# from vaderSentiment_1 import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "def sentiment_analysis(text):\n",
    "    analyzer = SentimentIntensityAnalyzer_1(lexicon_dict,emojis_dict)\n",
    "    return analyzer.polarity_scores(text)['compound'] ## only chec 'compound'\n",
    "\n",
    "sentiment_analysis_udf = udf(sentiment_analysis , FloatType())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = dfbins_en.select('weeks','song_name','artist_name','text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+------------+-----------+\n",
      "|weeks|song_name| artist_name|senti_score|\n",
      "+-----+---------+------------+-----------+\n",
      "|  0.0|     null|         BTS|        1.0|\n",
      "|  2.0|     null|         BTS|        1.0|\n",
      "|  2.0|     null|         BTS|        1.0|\n",
      "|  2.0|     null|         BTS|        1.0|\n",
      "|  2.0|     null|         BTS|        1.0|\n",
      "|  2.0|     null|         BTS|        1.0|\n",
      "|  2.0|     null|         BTS|        1.0|\n",
      "|  7.0|     null|         BTS|        1.0|\n",
      "| 12.0|     null|         BTS|        1.0|\n",
      "|  5.0|     null|Harry Styles|        1.0|\n",
      "| 11.0|     null|      CORPSE|        1.0|\n",
      "|  2.0|     null|         BTS|        1.0|\n",
      "|  2.0|     null|         BTS|        1.0|\n",
      "|  8.0|     null|Harry Styles|        1.0|\n",
      "|  2.0|     null|         BTS|        1.0|\n",
      "|  9.0|     null|         BTS|        1.0|\n",
      "|  9.0|     null|Harry Styles|        1.0|\n",
      "|  5.0|     null|         BTS|        1.0|\n",
      "|  2.0|     null|         BTS|        1.0|\n",
      "|  2.0|     null|         BTS|        1.0|\n",
      "+-----+---------+------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp = temp.withColumn('senti_score',sentiment_analysis_udf('text'))\n",
    "temp.select('weeks','song_name','artist_name','senti_score').sort('senti_score',ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------+------------------+\n",
      "|weeks|           song_name|   artist_name|  avg(senti_score)|\n",
      "+-----+--------------------+--------------+------------------+\n",
      "| 11.0|              Wonder|      Lil Baby|0.9975000023841858|\n",
      "|  0.0|                 Run|Brett Eldredge|0.9937000274658203|\n",
      "|  0.0|Beer Never Broke ...|    Luke Combs| 0.991599977016449|\n",
      "|  6.0|              Wonder|          Vedo|0.9889000058174133|\n",
      "|  2.0|               Roses|  Taylor Swift|0.9878000020980835|\n",
      "|  2.0|               Hello|  Taylor Swift|0.9871000051498413|\n",
      "|  8.0|               Intro|    Luke Combs|0.9850999712944031|\n",
      "|  8.0|             HOLIDAY|         Drake|0.9848999977111816|\n",
      "|  6.0|              lovely|           SZA|0.9847000241279602|\n",
      "|  7.0|              Slidin|         Topic|0.9814000129699707|\n",
      "|  9.0|   How You Like That|        Future|0.9812999963760376|\n",
      "|  0.0|               Hello|     Lady Gaga|0.9796000123023987|\n",
      "|  0.0|                Body| Ariana Grande|0.9787999987602234|\n",
      "|  2.0|       Hit Different|          ZAYN|0.9761000275611877|\n",
      "|  9.0|              lovely|          ZAYN|0.9760000109672546|\n",
      "| 11.0|              lovely|        Clairo|0.9753000140190125|\n",
      "| 10.0|               Movie|       Wallows| 0.974399983882904|\n",
      "|  1.0|            Daylight|            CJ|0.9715999960899353|\n",
      "|  1.0|           ALWAYS DO|           BTS|0.9700999855995178|\n",
      "|  0.0|              Wolves| Ariana Grande|0.9697999954223633|\n",
      "+-----+--------------------+--------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# df_emoji.groupBy(['song_name','weeks']).agg(F.mean('emoji_score'), F.count('emoji_score')\n",
    "#                                            ).sort('avg(emoji_score)',ascending=False).show()\n",
    "\n",
    "spotify_senti = temp.groupby('weeks','song_name','artist_name').agg(F.mean('senti_score')\n",
    "                                                   ).sort('avg(senti_score)',ascending=False)\n",
    "spotify_senti.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "S = spotify_senti.toPandas()\n",
    "S.to_csv('/home/wusean/spotify_senti.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
